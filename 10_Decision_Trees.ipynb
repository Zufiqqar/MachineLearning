{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10 Decision Trees",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zufiqqar/MachineLearning/blob/main/10_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vo7FzBojhyF"
      },
      "source": [
        "Available at https://www.comp.nus.edu.sg/~winkler/IT3011/decision-trees.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvf81x6Vrysn"
      },
      "source": [
        "<h1>NUS IT3011 Machine Learning and Applications</h1>\n",
        "\n",
        "<h2>Decision Trees</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NygwyV0D6f3H"
      },
      "source": [
        "In the notebook, we implement a decision tree from scratch and see how well it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaIyFkAgKS2X"
      },
      "source": [
        "\n",
        "---\n",
        "# 1 Implement Decision Tree\n",
        "\n",
        "We are going to implement a decision tree that can classify whether a salary of a person is above 50k USD per year or not (so it is a 2-class classification).\n",
        "\n",
        "We will use the adult dataset (a.k.a. census income dataset) from UCI: https://archive.ics.uci.edu/ml/datasets/adult ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnL9siKhTc4m"
      },
      "source": [
        "import csv\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7Ij6LRbftu"
      },
      "source": [
        "### .a Generate Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zHhH8kYUQ7X"
      },
      "source": [
        "We are going to explore the dataset. We are going to make use of 14 _attributes_ (also known as _features_ or _input dimensions_, but for this decision trees to be consistent with the algorithm, we'll use \"attributes\"). The attributes at index \\[0, 2, 4, 10, 11, 12] are continuous. We are going to discretize the continuous features, where values **larger** than the mean is assigned **1** and values **smaller** than the mean is assigned **0**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDHpbbmPLaos"
      },
      "source": [
        "def generate_data(data_type):\n",
        "  \"\"\"\n",
        "   Args:\n",
        "     data_type(String): \"train\" or \"test\"\n",
        "      \n",
        "   Returns:\n",
        "     data (np array) : contains the dataset loaded from csv file\n",
        "  \"\"\"\n",
        "  attributes = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-info_gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "     \n",
        "  if(data_type == 'train'):\n",
        "    data = pd.read_csv(\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
        "    names=attributes,\n",
        "    sep=r'\\s*,\\s*',\n",
        "    engine='python',\n",
        "    na_values=\"?\")\n",
        "  else:\n",
        "    data = pd.read_csv(\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
        "    names=attributes,\n",
        "    sep=r'\\s*,\\s*',\n",
        "    skiprows=[0],\n",
        "    engine='python',\n",
        "    na_values=\"?\")\n",
        "    \n",
        "  data = data.values\n",
        "  data = np.array(data)\n",
        "  \n",
        "  continuous_index = [0, 2, 4, 10, 11, 12]\n",
        "\n",
        "  for j in continuous_index:\n",
        "    mean_value=np.mean(data[:, j].astype(float))\n",
        "    for i in range(data.shape[0]):\n",
        "      if(data[i, j] > mean_value):\n",
        "        data[i, j] = '1'\n",
        "      else:\n",
        "        data[i, j] = '0'\n",
        "       \n",
        "  return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw6DkKBLXxUF"
      },
      "source": [
        "sample_data = generate_data(\"train\")[:10]\n",
        "pd.DataFrame(sample_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLllLWGHbjzQ"
      },
      "source": [
        "### .b Choose Feature Based on Majority"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5_7oL4ZcjGo"
      },
      "source": [
        "Next we are going to build a list of helper functions to help build the decision tree. Your task is to implement these functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl8HjOq1cssh"
      },
      "source": [
        "**Majority Class Function**: When you reach a terminal state in a decision tree, we return the value that represents the majority of the (sub)-dataset's instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itvosJdYeq-7"
      },
      "source": [
        "**Yout Turn (Question 3):** Complete the code below to find the majority class from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYAIbP-scrhP"
      },
      "source": [
        "def major_class(data):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      data(m, K): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          K: num_features + 1 (last column is the label)\n",
        "    Returns:\n",
        "      major: Type:String. The major class of this sub-dataset(e.g \">50K\" or \"<=50K\")\n",
        "    \"\"\"\n",
        "    major = \"\"\n",
        "    \n",
        "    # freq will store the number of occurences of the target labels\n",
        "    freq = {}\n",
        "    for entry in data:\n",
        "        if (entry[-1] in freq):\n",
        "            freq[entry[-1]] += 1.0\n",
        "        else:\n",
        "            freq[entry[-1]]  = 1.0\n",
        "            \n",
        "    major = \"\"\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q3): Write your code here\n",
        "    # Hint: Loop through each row of data, get the last column, then find which value occurs the most frequently.\n",
        "    #\n",
        "    ###########################\n",
        "\n",
        "    return major\n",
        "  \n",
        "\n",
        "# Testing: This should return '<=50K'\n",
        "major_class(sample_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89P0znHdGBh"
      },
      "source": [
        "### .c Calculate Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otRCzP-5e5kt"
      },
      "source": [
        "**Entropy Function**: Calculate the entropy of this current (sub-)dataset:\n",
        "\n",
        "Recall: $H(X) = - \\sum_{i=0}^C p_i\\log(p_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8XYG3k5e3_0"
      },
      "source": [
        "**Your Turn (Question 4):** Complete the code below to calculate entropy of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GYWOkzefeV7"
      },
      "source": [
        "def entropy(data):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      data(m, K): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          K: num_features + 1 (last column is the label)\n",
        "    Returns:\n",
        "      dataEntropy: The entropy of this current sub-dataset\n",
        "    \"\"\"\n",
        "    dataEntropy = 0.0\n",
        "    # freq will store the number of occurences of the target labels\n",
        "    freq = {}\n",
        "    for entry in data:\n",
        "        if (entry[-1] in freq):\n",
        "            freq[entry[-1]] += 1.0\n",
        "        else:\n",
        "            freq[entry[-1]]  = 1.0\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q4): Write your code here\n",
        "    # Hint: Loop through each row of data, get the last column, find number of occurences of each value, then use the above equation.\n",
        "    #\n",
        "    ###########################\n",
        "    \n",
        "    return dataEntropy\n",
        "\n",
        "# Testing: This should return 0.8812908992306927 (in log 2)\n",
        "entropy(sample_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R01dzi2VdKfj"
      },
      "source": [
        "### .d Calculate Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10aADF6ggK73"
      },
      "source": [
        "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
        "\n",
        "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen feature $x_i$ divides the example set S into subsets\n",
        "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
        "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
        "\n",
        "<div align=\"center\">\n",
        "$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$\n",
        "</div>\n",
        "\n",
        "Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$. We choose the attribute with the largest IG:\n",
        "<div align=\"center\">\n",
        "$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJuTNp6dfBHF"
      },
      "source": [
        "**Your Turn (Question 5):** Complete the code below to calculate information gain of a given attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZinWF-_gJ-J"
      },
      "source": [
        "def info_gain(data, attribute, attributes):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      data(m, K): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          K: num_features + 1 (last column is the label)\n",
        "      attribute: The attribute used to split data\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      info_gain : information gain of the given dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    freq = {}\n",
        "    subsetEntropy = 0.0\n",
        "    \n",
        "    # Get the column index of this attribute\n",
        "    i = attributes.index(attribute)\n",
        "    \n",
        "    for entry in data:\n",
        "        if (entry[i] in freq):\n",
        "            freq[entry[i]] += 1.0\n",
        "        else:\n",
        "            freq[entry[i]]  = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q5): Write your code here\n",
        "    # Hint: Split the data based on the value at index i. Find the subsetEntropy of\n",
        "    # each sub-dataset, then use the formula of Information Gain to calculate subsetEntropy.\n",
        "    #\n",
        "    ###########################\n",
        "    return (entropy(data) - subsetEntropy)\n",
        "\n",
        "# Testing: This should return 0.0058021490143458365\n",
        "attributes = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-info_gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "info_gain(sample_data, 'age', attributes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Hr9cZEdgxa"
      },
      "source": [
        "### .e Find the Best Attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMBTDnINti16"
      },
      "source": [
        "Now we will write a function to choose the **best** attribute for a given data, here best indicates having **largest** information gain among all attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A6RbEGjfOB0"
      },
      "source": [
        "**Your Turn (Question 6):** Complete the code below to get the best attribute based on information gain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pKj7RPCjMwa"
      },
      "source": [
        "def attr_choose(data, attributes):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      data(m, K): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          K: num_features + 1 (last column is the label)\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      best: The best attribute to split based on info gain.\n",
        "    \"\"\"\n",
        "    best = attributes[0]\n",
        "    \n",
        "    maxGain = 0\n",
        "    for attr in attributes[:-1]:\n",
        "      ###########################\n",
        "      #\n",
        "      # Your Turn (Q6): Write your code here\n",
        "      # Hint: For each attribute in attributes, use Info_gain function above\n",
        "      # to know which attribute is the best option\n",
        "      #\n",
        "      ###########################\n",
        "    return best\n",
        "  \n",
        "# Testing: This should return 'hours-per-week'\n",
        "attributes = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-info_gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "attr_choose(sample_data, attributes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_LD6EDddlm4"
      },
      "source": [
        "### .f Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ribCJTuEV6"
      },
      "source": [
        "We will define two helper functions here. First, `get_unique_values` returns the unique values of a given attribute. The second function, `get_data` returns the rows containing a specific value of a chosen attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIk0yhIdk0_j"
      },
      "source": [
        "# These two functions are helper functions\n",
        "# This function will get unique values for that particular attribute from the given data\n",
        "def get_unique_values(data, attributes, attribute):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,K) : Current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      attribute : Our point of interest\n",
        "    \n",
        "    Returns:\n",
        "      values : List of unique values for our point of interest\n",
        "    \n",
        "    \"\"\"\n",
        "    index = attributes.index(attribute)\n",
        "    values = []\n",
        "    # \n",
        "    for entry in data:\n",
        "        if entry[index] not in values:\n",
        "            values.append(entry[index])\n",
        "\n",
        "    return values\n",
        "\n",
        "# This function will get all the rows of the data where the chosen \"best\" attribute has a value \"val\"\n",
        "def get_data(data, attributes, best, val):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,K) : Current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      best : The attribute of which data we will extract\n",
        "      val : We are interested only on this value of the `best` attribute\n",
        "    Returns:\n",
        "      new_data : Data subset containing only those rows where `best` attribute = val\n",
        "    \n",
        "    \"\"\"\n",
        "    new_data = [[]]\n",
        "    index = attributes.index(best)\n",
        "\n",
        "    for entry in data:\n",
        "        if (entry[index] == val):\n",
        "            newEntry = []\n",
        "            for i in range(0,len(entry)):\n",
        "                if(i != index):\n",
        "                    newEntry.append(entry[i])\n",
        "            new_data.append(newEntry)\n",
        "\n",
        "    new_data.remove([])    \n",
        "    return new_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWARfhFPdrN2"
      },
      "source": [
        "### .g Build Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzghFfmKsah_"
      },
      "source": [
        "In the below code, your task is to implement the build a tree recursively. Starting at the root, pick the best attribute to split on, and call the `build_tree` function on each of the sub-trees.  Check the inlined code comments for clarification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cXPWX_PfXkD"
      },
      "source": [
        "**Your Turn (Question 7):** Complete the code below to build the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkd-2ULtlHM2"
      },
      "source": [
        "def build_tree(data, attributes):\n",
        "    \"\"\" \n",
        "      Args:\n",
        "        data(m, K): The current sub-dataset of the sub-tree\n",
        "            m: num_rows\n",
        "            K: num_features + 1 (last column is the label)\n",
        "        attributes: The list of current remaining attributes\n",
        "      Returns:\n",
        "        tree: The constructed tree as object. For example if the root is gender,\n",
        "              then a tree of depth 2 is like \n",
        "              {'gender': {'male': sub_tree1, 'female': sub_tree2}}\n",
        "    \"\"\"\n",
        "    data = data[:]\n",
        "    \n",
        "    # vals is the labels of the records in data.\n",
        "    vals = [record[-1] for record in data]\n",
        "    \n",
        "    \n",
        "    ##################################################################\n",
        "    ## Your Turn (Q7): Finish the implementation of the below code ##\n",
        "    if (len(data) == 0) or (len(attributes) - 1) <= 0: # What should we return if we run out of features to split?\n",
        "        return #replace with your answer\n",
        "    elif vals.count(vals[0]) == len(vals): # What should we return if this sub-dataset is pure?\n",
        "        return #replace with your answer\n",
        "    else:\n",
        "        best = attr_choose(data, attributes)\n",
        "        tree = {best:{}}\n",
        "    \n",
        "        for val in get_unique_values(data, attributes, best):\n",
        "            ### Calculate the subtree here\n",
        "            #\n",
        "            # Write your code here\n",
        "            #\n",
        "            tree[best][val] = subtree\n",
        "    \n",
        "    return tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2KC5frjw87D"
      },
      "source": [
        "The below code block containing `run_decision_tree` does exactly as its name, and you don't have to understand its code for the purpose of this exercise. Just run it and check the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GzCNFXIlngj"
      },
      "source": [
        "# Class Node which will be used while classify a test-instance using the tree which was built earlier\n",
        "class Node():\n",
        "    value = \"\"\n",
        "    children = []\n",
        "\n",
        "    def __init__(self, val, dictionary):\n",
        "        self.value = val\n",
        "        if (isinstance(dictionary, dict)):\n",
        "            self.children = list(dictionary.keys())\n",
        "\n",
        "def run_decision_tree():\n",
        "    \n",
        "#     data = generate_data()\n",
        "    attributes = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-info_gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "    acc = []\n",
        "    training_set = generate_data(\"train\")\n",
        "    test_set = generate_data(\"test\")\n",
        "    tree = build_tree(training_set, attributes)\n",
        "    results = []\n",
        "\n",
        "    for entry in test_set:\n",
        "        tempDict = tree.copy()\n",
        "        result = \"\"\n",
        "        while(isinstance(tempDict, dict)):\n",
        "            root = Node(list(tempDict.keys())[0], tempDict[list(tempDict.keys())[0]])\n",
        "            tempDict = tempDict[list(tempDict.keys())[0]]\n",
        "            index = attributes.index(root.value)\n",
        "            value = entry[index]\n",
        "            if(value in list(tempDict.keys())):\n",
        "                child = Node(value, tempDict[value])\n",
        "                result = tempDict[value]\n",
        "                tempDict = tempDict[value]\n",
        "            else:\n",
        "                result = \"Null\"\n",
        "                break\n",
        "        if result != \"Null\":\n",
        "            results.append(result == entry[-1][:-1])\n",
        "        \n",
        "    accuracy = float(results.count(True))/float(len(results))\n",
        "    print(results)\n",
        "    print(\"FINAL ACCURACY\")\n",
        "    print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38y6Od4gn2mM"
      },
      "source": [
        "# run decision tree to get the accuracy\n",
        "run_decision_tree()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCim-XDIYvhN"
      },
      "source": [
        "# 2 Visualize Decision Tree Boundaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019c6M1rK4gs"
      },
      "source": [
        "Let's apply Decision Trees, Random Forests and the AdaBoost classifiers with\n",
        "Decision Stumps to classify on the Iris dataset. We'll visualize the decision boundary as well.\n",
        "\n",
        "For the sake of visualization, we will choose subsets of two features from  the Iris dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIk4lhZQY-ji"
      },
      "source": [
        "### .a Code to be run\n",
        "\n",
        "The code block below can be safely folded away and run.  While you're welcome to study the code, it is not necessary to understand it entirely. All you need to do is to modify the below variable `trial_pairs` to play around with the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjNDWBR-z4vL"
      },
      "source": [
        "##################################################################\n",
        "### Your Turn: modify and try different feature combinations  ####\n",
        "trial_pairs = ([0, 1], [0, 2], [2, 3])\n",
        "\n",
        "##################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU12Yk9N0FoE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn import clone\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                              AdaBoostClassifier)\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmrPAaUW0HI_"
      },
      "source": [
        "# You may have to change these parameters later, to answer Q5-6\n",
        "# Parameters\n",
        "n_classes = 3\n",
        "n_estimators = 30  # no of estimaotr for random forest and adaboost\n",
        "cmap = plt.cm.RdYlBu\n",
        "plot_step = 0.02  # fine step width for decision surface contours\n",
        "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
        "RANDOM_SEED = 13  # fix the seed on each iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0KsTY9q0Jxo"
      },
      "source": [
        "# Load data\n",
        "iris = load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1-9u9IN0KZf"
      },
      "source": [
        "models = [DecisionTreeClassifier(max_depth=None),\n",
        "          RandomForestClassifier(n_estimators=n_estimators),\n",
        "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),\n",
        "                             n_estimators=n_estimators)]\n",
        "\n",
        "plot_idx = 1\n",
        "for pair in trial_pairs:\n",
        "    X = iris.data[:, pair]\n",
        "    y = iris.target\n",
        "    \n",
        "    plt.subplot(1, 3, plot_idx)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y,\n",
        "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
        "                    edgecolor='k', s=20)\n",
        "    plt.title(pair)\n",
        "    plot_idx += 1  # move on to the next plot in sequence\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPZauA7a0Mxh"
      },
      "source": [
        "plot_idx = 1\n",
        "\n",
        "\n",
        "#######################################\n",
        "for pair in trial_pairs:\n",
        "    for model in models:\n",
        "      \n",
        "        # We only take the two corresponding features\n",
        "        X = iris.data[:, pair]\n",
        "        y = iris.target\n",
        "\n",
        "        # Shuffle\n",
        "        idx = np.arange(X.shape[0])\n",
        "        np.random.seed(RANDOM_SEED)\n",
        "        np.random.shuffle(idx)\n",
        "        X = X[idx]\n",
        "        y = y[idx]\n",
        "\n",
        "        # Standardize\n",
        "        mean = X.mean(axis=0)\n",
        "        std = X.std(axis=0)\n",
        "        X = (X - mean) / std\n",
        "        \n",
        "        # Train\n",
        "        clf = clone(model)\n",
        "        clf = model.fit(X, y)\n",
        "        scores = clf.score(X, y)\n",
        "        \n",
        "        # Create a title for each column and the console by using str() and\n",
        "        # slicing away useless parts of the string\n",
        "        model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
        "        model_details = model_title\n",
        "        if hasattr(model, \"estimators_\"):\n",
        "            model_details += \" with {} estimators\".format(\n",
        "                len(model.estimators_))\n",
        "        print(model_details + \" with features\", pair,\n",
        "              \"has a score of\", scores)\n",
        "\n",
        "        plt.subplot(3, 3, plot_idx)\n",
        "        if plot_idx <= len(models):\n",
        "            # Add a title at the top of each column\n",
        "            plt.title(model_title)\n",
        "\n",
        "        # Now plot the decision boundary using a fine mesh as input to a\n",
        "        # filled contour plot\n",
        "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                             np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
        "        # decision surfaces of the ensemble of classifiers\n",
        "        if isinstance(model, DecisionTreeClassifier):\n",
        "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
        "        else:\n",
        "            # Choose alpha blend level with respect to the number\n",
        "            # of estimators\n",
        "            # that are in use (noting that AdaBoost can use fewer estimators\n",
        "            # than its maximum if it achieves a good enough fit early on)\n",
        "            estimator_alpha = 1.0 / len(model.estimators_)\n",
        "            for tree in model.estimators_:\n",
        "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "                Z = Z.reshape(xx.shape)\n",
        "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
        "\n",
        "        # Build a coarser grid to plot a set of ensemble classifications\n",
        "        # to show how these are different to what we see in the decision\n",
        "        # surfaces. These points are regularly space and do not have a\n",
        "        # black outline\n",
        "        xx_coarser, yy_coarser = np.meshgrid(\n",
        "            np.arange(x_min, x_max, plot_step_coarser),\n",
        "            np.arange(y_min, y_max, plot_step_coarser))\n",
        "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
        "                                         yy_coarser.ravel()]\n",
        "                                         ).reshape(xx_coarser.shape)\n",
        "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
        "                                c=Z_points_coarser, cmap=cmap,\n",
        "                                edgecolors=\"none\")\n",
        "\n",
        "        # Plot the training points, these are clustered together and have a\n",
        "        # black outline\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
        "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
        "                    edgecolor='k', s=20)\n",
        "        plot_idx += 1  # move on to the next plot in sequence\n",
        "\n",
        "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\")\n",
        "plt.axis(\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq_TNQTaZQEF"
      },
      "source": [
        "### .b Questions for your understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0Uxtn9EUrK"
      },
      "source": [
        "**Your Turn (Question 5):** From the visualization above, is Random Forest doing better than Decision Tree? Will the result still be the same, or improve if we have less estimators (say $10$) for Random Forest?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXI2qeZqF0b7"
      },
      "source": [
        "**Your Turn (Question 6):** You might have found, we use `RANDOM_SEED = 13` to get a deterministic result for everyone. What if we change the seed to a different value, say `RANDOM_SEED = 119`. Will the performance of three classifiers, decision tree, random forest and adaboost will still be same? If not, why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlkWY_70go18"
      },
      "source": [
        "**Your Turn (Question 7):**  There are other feature pairs in the Iris dataset. Experiment with these additional feature pairs to see which group of features perfom best among all pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kGaO2wXmyVP"
      },
      "source": [
        "---\n",
        "# Credits\n",
        "Authored by Le Trung Hieu, Mohammad Neamul Kabir, Aadyaa Maddi and [Min-Yen Kan](http://www.comp.nus.edu.sg/~kanmy) (2019), affiliated with [WING](http://wing.comp.nus.edu.sg), [NUS School of Computing](http://www.comp.nus.edu.sg) and [ALSET](http://www.nus.edu.sg/alset).\n",
        "Licensed as: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/ ) (CC BY 4.0).\n",
        "Please retain and add to this credits cell if using this material as a whole or in part.\n",
        "Other Credits (inclusive of photos): "
      ]
    }
  ]
}